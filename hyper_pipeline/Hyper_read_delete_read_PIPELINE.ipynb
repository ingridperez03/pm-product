{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743fac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tableauhyperapi in c:\\users\\aldri\\anaconda3\\lib\\site-packages (0.0.13821)\n",
      "Requirement already satisfied: cffi!=1.14.3,<2,>=1.12.2 in c:\\users\\aldri\\anaconda3\\lib\\site-packages (from tableauhyperapi) (1.14.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aldri\\anaconda3\\lib\\site-packages (from cffi!=1.14.3,<2,>=1.12.2->tableauhyperapi) (2.20)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install tableauhyperapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7fe6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tableauhyperapi import HyperProcess, Telemetry, \\\n",
    "    Connection, CreateMode, \\\n",
    "    NOT_NULLABLE, NULLABLE, SqlType, TableDefinition, \\\n",
    "    Inserter, \\\n",
    "    escape_name, escape_string_literal, \\\n",
    "    TableName, \\\n",
    "    HyperException, \\\n",
    "    Nullability\n",
    "\n",
    "import tableauhyperapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ec59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3bda747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An example of how to read and print data from an existing Hyper file.\n",
    "def read1():\n",
    "    # Path to a Hyper file containing all data inserted into Customer, Product, Orders and LineItems table.\n",
    "    path_to_source_database = \"Data.hyper\"\n",
    "\n",
    "    # Make a copy of the superstore denormalized sample Hyper file\n",
    "    #path_to_database = Path(shutil.copy(src=path_to_source_database, dst=\"NONE.hyper\")).resolve()\n",
    "\n",
    "    # Starts the Hyper Process with telemetry enabled to send data to Tableau.\n",
    "    # To opt out, simply set telemetry=Telemetry.DO_NOT_SEND_USAGE_DATA_TO_TABLEAU.\n",
    "    with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper:\n",
    "\n",
    "        # Connect to existing Hyper file \"superstore_sample_denormalized_read.hyper\".\n",
    "        with Connection(endpoint=hyper.endpoint, database=path_to_source_database) as connection:\n",
    "            \n",
    "            # The `connection.catalog` provides us with access to the meta-data we are interested in\n",
    "            catalog = connection.catalog\n",
    "\n",
    "            # Iterate over all schemas and print them\n",
    "            schemas = catalog.get_schema_names()\n",
    "            print(f\"{len(schemas)} schemas:\")\n",
    "            for schema_name in schemas:\n",
    "                # For each schema, iterate over all tables and print them\n",
    "                tables = catalog.get_table_names(schema=schema_name)\n",
    "                print(f\" * Schema {schema_name}: {len(tables)} tables\")\n",
    "                for table in tables:\n",
    "                    # For each table, iterate over all columns and print them\n",
    "                    table_definition = catalog.get_table_definition(name=table)\n",
    "                    print(f\"  -> Table {table.name}: {len(table_definition.columns)} columns\")\n",
    "                    for column in table_definition.columns:\n",
    "                        nullability = \" NOT NULL\" if column.nullability == Nullability.NOT_NULLABLE else \"\"\n",
    "                        collation = \" \" + column.collation if column.collation is not None else \"\"\n",
    "                        print(f\"    -> {column.name} {column.type}{nullability}{collation}\")\n",
    "\n",
    "        print(\"\\nThe connection to the Hyper file has been closed.\")\n",
    "    print(\"The Hyper process has been shut down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a1864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An example of how to delete data in an existing Hyper file.\n",
    "def delete():\n",
    "\n",
    "    # Path to a Hyper file containing all data inserted into Customer, Product, Orders and LineItems table.\n",
    "    path_to_source_database = \"Data.hyper\"\n",
    "    \n",
    "    # Make a copy of the superstore example Hyper file.\n",
    "    path_to_database = Path(shutil.copy(src=path_to_source_database, dst=\"DataTEMP1_deleted.hyper\")).resolve()\n",
    "\n",
    "    # Starts the Hyper Process with telemetry enabled to send data to Tableau.\n",
    "    # To opt out, simply set telemetry=Telemetry.DO_NOT_SEND_USAGE_DATA_TO_TABLEAU.\n",
    "    with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper:\n",
    "\n",
    "        # Connect to existing Hyper file \"superstore_sample_delete.hyper\".\n",
    "        with Connection(endpoint=hyper.endpoint, database=path_to_database) as connection:\n",
    "            \n",
    "            catalog = connection.catalog\n",
    "            \n",
    "            schemas = catalog.get_schema_names() #schemas[0] is \"public\", #schemas[1] is \"Extract\"\n",
    "            tables = catalog.get_table_names(schema=schemas[1]) #tables[0] is \"Extract\"\n",
    "            table_definition = catalog.get_table_definition(name=tables[0])\n",
    "            \n",
    "            Columns_to_delete = []\n",
    "            \n",
    "            #Municipalities Totals\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"% abstenció\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"% votants\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"% vots a candidatures\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"% vots en blancs\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"% vots nuls\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"% vots vàlids\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Abstenció\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Cens escrutat\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Codi circumscripció\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Codi municipi\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Votants\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Vots a candidatures\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Vots en blanc\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Vots nuls\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Vots vàlids\"))\n",
    "            \n",
    "            #Municipalities Parties\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"%\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Codi circumscripció1\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Codi comarca\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Codi municipi1\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Vots\"))\n",
    "            \n",
    "            #Extract\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Calculation_1171217395906875396\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Non-independency votes (copy)\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Number of Records\"))\n",
    "            \n",
    "            #dic16\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"CMUN\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"CPRO\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"HOMBRES\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"MUJERES\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"POB16\"))\n",
    "            \n",
    "            #Municipalities Parties\n",
    "            #Columns_to_delete.append(tableauhyperapi.name.Name(\"Nom comarca\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Nom municipi1\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Party\"))\n",
    "            \n",
    "            #MUC_TMB.shp\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"CODI_INE\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"ID\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"MUNICIPI\"))\n",
    "            \n",
    "            #Extract\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Calculation_1171217395889684480\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"ID\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"Party (group)\"))\n",
    "            \n",
    "            #dic16\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"NOMBRE\"))\n",
    "            Columns_to_delete.append(tableauhyperapi.name.Name(\"PROVINCIA\"))\n",
    "            \n",
    "            # execute_command executes a SQL statement and returns the impacted row count.\n",
    "            for column in table_definition.columns:\n",
    "                if (column.name in Columns_to_delete):\n",
    "                    row_count = connection.execute_command(command=f\"ALTER TABLE {tables[0]} \"\n",
    "                                                  f\"DROP COLUMN {column.name}\")\n",
    "                    \n",
    "            schemas = catalog.get_schema_names() #schemas[0] is \"public\", #schemas[1] is \"Extract\"\n",
    "            tables = catalog.get_table_names(schema=schemas[1]) #tables[0] is \"Extract\"\n",
    "            table_definition = catalog.get_table_definition(name=tables[0])\n",
    "\n",
    "        print(\"\\nThe connection to the Hyper file has been closed.\")\n",
    "    print(\"The Hyper process has been shut down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7bcade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An example of how to read and print data from an existing Hyper file.\n",
    "def read2():\n",
    "    \n",
    "    # Path to a Hyper file containing all data inserted into Customer, Product, Orders and LineItems table.\n",
    "    path_to_source_database = \"DataTEMP1_deleted.hyper\"\n",
    "\n",
    "    # Make a copy of the superstore denormalized sample Hyper file\n",
    "    #path_to_database = Path(shutil.copy(src=path_to_source_database, dst=\"NONE.hyper\")).resolve()\n",
    "\n",
    "    # Starts the Hyper Process with telemetry enabled to send data to Tableau.\n",
    "    # To opt out, simply set telemetry=Telemetry.DO_NOT_SEND_USAGE_DATA_TO_TABLEAU.\n",
    "    with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper:\n",
    "\n",
    "        # Connect to existing Hyper file \"superstore_sample_denormalized_read.hyper\".\n",
    "        with Connection(endpoint=hyper.endpoint, database=path_to_source_database) as connection:\n",
    "            \n",
    "            # The `connection.catalog` provides us with access to the meta-data we are interested in\n",
    "            catalog = connection.catalog\n",
    "\n",
    "            # Iterate over all schemas and print them\n",
    "            schemas = catalog.get_schema_names()\n",
    "            print(f\"{len(schemas)} schemas:\")\n",
    "            for schema_name in schemas:\n",
    "                # For each schema, iterate over all tables and print them\n",
    "                tables = catalog.get_table_names(schema=schema_name)\n",
    "                print(f\" * Schema {schema_name}: {len(tables)} tables\")\n",
    "                for table in tables:\n",
    "                    # For each table, iterate over all columns and print them\n",
    "                    table_definition = catalog.get_table_definition(name=table)\n",
    "                    print(f\"  -> Table {table.name}: {len(table_definition.columns)} columns\")\n",
    "                    for column in table_definition.columns:\n",
    "                        nullability = \" NOT NULL\" if column.nullability == Nullability.NOT_NULLABLE else \"\"\n",
    "                        collation = \" \" + column.collation if column.collation is not None else \"\"\n",
    "                        print(f\"    -> {column.name} {column.type}{nullability}{collation}\")\n",
    "\n",
    "        print(\"\\nThe connection to the Hyper file has been closed.\")\n",
    "    print(\"The Hyper process has been shut down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "752ff58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An example of how to delete data in an existing Hyper file.\n",
    "def reduction():\n",
    "\n",
    "    # Path to a Hyper file containing all data inserted into Customer, Product, Orders and LineItems table.\n",
    "    # See \"insert_data_into_multiple_tables.py\" for an example that works with the complete schema.\n",
    "    path_to_source_database = \"DataTEMP1_deleted.hyper\"\n",
    "\n",
    "    # Make a copy of the superstore example Hyper file.\n",
    "    path_to_database = Path(shutil.copy(path_to_source_database, \"DataTEMP2_reduced.hyper\")).resolve()\n",
    "\n",
    "    # Starts the Hyper Process with telemetry enabled to send data to Tableau.\n",
    "    # To opt out, simply set telemetry=Telemetry.DO_NOT_SEND_USAGE_DATA_TO_TABLEAU.\n",
    "    with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper:\n",
    "\n",
    "        # Connect to existing Hyper file \"superstore_sample_delete.hyper\".\n",
    "        with Connection(endpoint=hyper.endpoint, database=path_to_database) as connection:\n",
    "            \n",
    "            catalog = connection.catalog\n",
    "            \n",
    "            schemas = catalog.get_schema_names() #schemas[0] is \"public\", #schemas[1] is \"Extract\"\n",
    "            tables = catalog.get_table_names(schema=schemas[1]) #tables[0] is \"Extract\"\n",
    "            table_definition = catalog.get_table_definition(name=tables[0])\n",
    "            \n",
    "            # execute_command executes a SQL statement and returns the impacted row count.\n",
    "            # v1\n",
    "            Distinct_string = f\"(SELECT DISTINCT \"\n",
    "            for i in table_definition.columns:\n",
    "                Distinct_string+= f\"{i.name}, \"\n",
    "            Distinct_string = Distinct_string[:-2]\n",
    "            Distinct_string+= \" \"\n",
    "            \n",
    "            row_count = connection.execute_command(command=f\"CREATE TABLE {schemas[1]}.temp \"\n",
    "                                                   f\"AS \"\n",
    "                                                   +Distinct_string+\n",
    "                                                   f\"FROM {tables[0]})\")\n",
    "            \n",
    "            # v2\n",
    "            #row_count = connection.execute_command(command=f\"CREATE TABLE {schemas[1]}.temp \"\n",
    "                                                   #f\"AS \"\n",
    "                                                   #f\"(SELECT DISTINCT MIN({table_definition.columns[0].name}) AS {table_definition.columns[0].name}, {table_definition.columns[1].name} \"\n",
    "                                                   #f\"FROM {tables[0]} \"\n",
    "                                                   #f\"GROUP BY \"\n",
    "                                                   #f\"{table_definition.columns[1].name})\")\n",
    "            \n",
    "            row_count = connection.execute_command(command=f\"DROP TABLE {tables[0]} \")\n",
    "            \n",
    "            row_count = connection.execute_command(command=f\"CREATE TABLE {tables[0]} AS (SELECT * FROM {schemas[1]}.temp)\")\n",
    "            \n",
    "            row_count = connection.execute_command(command=f\"DROP TABLE {schemas[1]}.temp \")\n",
    "            \n",
    "        print(\"\\nThe connection to the Hyper file has been closed.\")\n",
    "    print(\"The Hyper process has been shut down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa635e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of how you can optimize the file storage of an existing `.hyper` file by copying \n",
    "# all of the tables and data into a new file. This reduces file fragmentation.\n",
    "\n",
    "def defragment():\n",
    "    with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper:\n",
    "        with Connection(endpoint=hyper.endpoint) as connection:\n",
    "            # Connect to the input and output databases\n",
    "            # Create the output Hyper file or overwrite it\n",
    "            catalog = connection.catalog\n",
    "            catalog.drop_database_if_exists(\"DataOut.hyper\")\n",
    "            catalog.create_database(\"DataOut.hyper\")\n",
    "            catalog.attach_database(\"DataOut.hyper\", alias=\"output_database\")\n",
    "            catalog.attach_database(\"DataTEMP2_reduced.hyper\", alias=\"input_database\")\n",
    "\n",
    "            # Process all tables of all schemas of the input Hyper file and copy them into the output Hyper file\n",
    "            for input_schema_name in catalog.get_schema_names(\"input_database\"):\n",
    "                for input_table_name in catalog.get_table_names(input_schema_name):\n",
    "                    output_table_name = TableName(\"output_database\", input_schema_name.name, input_table_name.name)\n",
    "                    output_table_definition = TableDefinition(output_table_name, catalog.get_table_definition(input_table_name).columns)\n",
    "                    catalog.create_schema_if_not_exists(output_table_name.schema_name)\n",
    "                    catalog.create_table(output_table_definition)\n",
    "                    connection.execute_command(f\"INSERT INTO {output_table_name} (SELECT * FROM {input_table_name})\")\n",
    "                    print(f\"Successfully converted table {input_table_name}\")\n",
    "            print(f\"Successfully converted into DataOut.hyper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910dea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################\n",
      "# READING ORIGINAL .hyper\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "# DELETE UNUSED COLUMNS OF THE .hyper\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "# REDUCING MODIFIED .hyper\n",
      "################################################################\n",
      "\n",
      "################################################################\n",
      "# DEFRAGMENTING MODIFIED .hyper\n",
      "################################################################\n",
      "\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        print(\"\\n################################################################\")\n",
    "        print(\"# READING ORIGINAL .hyper\")\n",
    "        print(\"################################################################\")\n",
    "        #read1()\n",
    "        \n",
    "        print(\"\\n################################################################\")\n",
    "        print(\"# DELETE UNUSED COLUMNS OF THE .hyper\")\n",
    "        print(\"################################################################\")\n",
    "        with HiddenPrints(): delete()\n",
    "        #read2()\n",
    "        \n",
    "        print(\"\\n################################################################\")\n",
    "        print(\"# REDUCING MODIFIED .hyper\")\n",
    "        print(\"################################################################\")\n",
    "        with HiddenPrints(): reduction()\n",
    "        #read2()\n",
    "        \n",
    "        print(\"\\n################################################################\")\n",
    "        print(\"# DEFRAGMENTING MODIFIED .hyper\")\n",
    "        print(\"################################################################\")\n",
    "        with HiddenPrints(): defragment()\n",
    "        #read2()\n",
    "        \n",
    "        print(\"\\nSUCCESS\")\n",
    "        \n",
    "    except HyperException as ex:\n",
    "        print(ex)\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce4278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
